{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0Jxir3EIqWr"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# getting paths of directories having images of \"with_mask\" & \"without_mask\"\n",
        "path=\"dataset\"\n",
        "data=os.listdir(path)\n",
        "data=data[:-1]\n",
        "\n",
        "target=[i for i in range (len(data))]\n",
        "dictt=dict(zip(data,Responces))                            \n",
        "                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p05FKbWU4CO-"
      },
      "outputs": [],
      "source": [
        "# Normalizing the data sets, dataset consist of 3 colors RGB, so simply we convertthem into grayscale images which are easy to processible\n",
        "# diffrenet images have diffrent size so we resize imges.\n",
        "\n",
        "test_data=[]                     \n",
        "output=[]             \n",
        "for i in data:\n",
        "    dataFolder = os.path.join(path,i)\n",
        "    images_list = os.listdir(dataFolder)  \n",
        "    count=0\n",
        "    for img in images_list:                        \n",
        "        path_image = os.path.join(dataFolder,img)\n",
        "        image = cv2.imread(path_image)\n",
        "        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)   \n",
        "        res_image = cv2.resize(gray,(100,100))      \n",
        "        test_data.append(res_image)\n",
        "        output.append(Responces_dictioanry[i])    \n",
        "#using numpy to turn our to arrays and like matrixes so can be used in CNN models \n",
        "test_data=np.array(test_data)/255.0     \n",
        "test_data=np.reshape(test_data,(test_data.shape[0],100,100,1)) #reshapes matrices \n",
        "output=np.array(output)\n",
        "\n",
        "# converting into catagorical representation since at outer-layer have 2 neurons to output                                                               \n",
        "output=np_utils.to_categorical(output) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k60zIq9dC-3f"
      },
      "outputs": [],
      "source": [
        "#for building MLp model, we are using keros\n",
        "\n",
        "model= Sequential()\n",
        "# we will use 2 convolutionary layers fallowed by other layers\n",
        "\n",
        "# this is first convolutionary layer, we set filters=200, kernal_size=(3,3), activation function 'relu'.\n",
        "\n",
        "model.add(Conv2D(filters=200,kernel_size=(3,3),input_shape=test_data.shape[1:]))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# first layer is followed by maxpooling layer\n",
        "\n",
        "\n",
        "#second convolutionary layer, we set filters=200, kernal_size=(3,3), activation function 'relu'.\n",
        "model.add(Conv2D(filters=100,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        " \n",
        "#1 dimonshional list \n",
        "model.add(Flatten())\n",
        "\n",
        "#for overfitting we use dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(50,activation='relu'))\n",
        "\n",
        "#the final output layer with two outputs for two catogaries\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "\n",
        "# Now we compile the model using 'categorical_crossentropy' loss, optimizer 'adam' and 'accuracy' as a metric\n",
        "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can see an overview of the model you built using .summary()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8BSoXdxnycYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmJJceYsGuiH"
      },
      "outputs": [],
      "source": [
        "## We divide our data into test and train sets with 80% training size\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,Y_train,X_test,Y_test=train_test_split(test_data,output,test_size=0.2)\n",
        "\n",
        "# We fit the model, and save it to a variable 'history' that can be accessed later to analyze the training profile\n",
        "# We also set validation_split=0.2 for 20% of training data to be used for validation\n",
        "# verbose=0 means you will not see the output after every epoch. \n",
        "# we set batch_size =16 its means that the numbers of training examples utilized in one iteration\n",
        "# epochs set 20 mean how many time train data \n",
        "\n",
        "history=model.fit(X_train,Y_train,batch_size=16,epochs=20,validation_split=0.2,verbose='auto')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graphs of training and validation accuracy\n",
        "fig, ax = plt.subplots(1,2,figsize = (16,4))\n",
        "ax[0].plot(history.history['loss'],color='#EFAEA4',label = 'Training Loss')\n",
        "ax[0].plot(history.history['val_loss'],color='#B2D7D0',label = 'Validation Loss')\n",
        "ax[1].plot(history.history['accuracy'],color='#EFAEA4',label = 'Training Accuracy')\n",
        "ax[1].plot(history.history['val_accuracy'],color='#B2D7D0',label = 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[1].set_xlabel('Epochs');\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_ylabel('Accuracy %');\n",
        "fig.suptitle('MLP Training', fontsize = 24)"
      ],
      "metadata": {
        "id": "TpyT6lQDYynS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evulating our model\n",
        "model.evaluate(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrBggN1k3jh_",
        "outputId": "71d0596e-c979-42e9-9ca2-871673318006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 35ms/step - loss: 0.3444 - accuracy: 0.9457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34441259503364563, 0.945652186870575]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Face_mask_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}